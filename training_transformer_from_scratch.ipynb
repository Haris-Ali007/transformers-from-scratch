{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Haris-Ali007/transformers-from-scratch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDcNSp8WnY9N",
        "outputId": "e655fc54-1e93-4e6a-f380-fa51ba4e0ff7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers-from-scratch'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 23 (delta 8), reused 17 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (23/23), 6.44 KiB | 6.44 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "3511_-r78w2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text\n",
        "from utils import masked_accuracy, masked_loss, CustomScheduler\n",
        "from model import Transformer\n",
        "import argparse\n",
        "\n",
        "def prepare_batch(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)\n",
        "    pt = pt[:, :MAX_TOKENS]\n",
        "    pt = pt.to_tensor()\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)] # all rows MAX_TOKENS size cols\n",
        "    en_true = en[:, :-1].to_tensor() # DROP end token\n",
        "    en_label = en[:, 1:].to_tensor() # DROP start token\n",
        "    return (pt, en_true), en_label\n",
        "\n",
        "\n",
        "def make_batches(ds):\n",
        "    return (ds\n",
        "            .shuffle(BUFFER_SIZE)\n",
        "            .batch(BATCH_SIZE)\n",
        "            .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "            .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    ### CONFIGURATIONS\n",
        "    MAX_TOKENS=128\n",
        "    BUFFER_SIZE = 20000\n",
        "    num_layers=4\n",
        "    d_model=128\n",
        "    dff=512\n",
        "    num_heads=8\n",
        "    dropout_rate=0.1\n",
        "    BATCH_SIZE = 8\n",
        "    TRAINING_EPOCHS = 10\n",
        "    MODEL_SAVE_PATH = 'transformer_model'\n",
        "\n",
        "    #### Downloading dataset\n",
        "    examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                                with_info=True,\n",
        "                                as_supervised=True)\n",
        "    train_examples, val_examples = examples['train'], examples['validation']\n",
        "    for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "        print('Examples in Portuguese')\n",
        "        for pt in pt_examples.numpy():\n",
        "            print(pt.decode('utf-8'))\n",
        "        print()\n",
        "\n",
        "        print('Examples in english')\n",
        "        for en in en_examples.numpy():\n",
        "            print(en.decode('utf-8'))\n",
        "\n",
        "    #### Pre processing\n",
        "    model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "    tf.keras.utils.get_file(\n",
        "                    f'{model_name}.zip',\n",
        "                    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "                    cache_dir='.', cache_subdir='', extract=True\n",
        "                )\n",
        "    tokenizers = tf.saved_model.load(model_name)\n",
        "    encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "    print('> Token ID batch')\n",
        "    for row in encoded.to_list():\n",
        "        print(row)\n",
        "\n",
        "    round_trip = tokenizers.en.detokenize(encoded)\n",
        "    print(f\"English Vocab size {tokenizers.en.get_vocab_size()}\")\n",
        "    print(f\"Portuguese Vocab size {tokenizers.pt.get_vocab_size()}\")\n",
        "\n",
        "    lengths = []\n",
        "    for pt_examples, en_examples in train_examples.batch(1).take(1):\n",
        "        pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "        lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "        en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "        lengths.append(en_tokens.row_lengths())\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "    train_batches = make_batches(train_examples)\n",
        "    val_batches = make_batches(val_examples)\n",
        "\n",
        "    transformer = Transformer(num_layers=num_layers, d_model=d_model,\n",
        "                            dff=dff, num_heads=num_heads,\n",
        "                            dropout_rate=dropout_rate,\n",
        "                            input_vocab_size=tokenizers.pt.get_vocab_size(),\n",
        "                            target_vocab_size=tokenizers.en.get_vocab_size())\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomScheduler(d_model))\n",
        "    transformer.compile(optimizer=optimizer,\n",
        "                        loss=masked_loss,\n",
        "                        metrics=[masked_accuracy])\n",
        "    transformer.fit(train_batches, epochs=TRAINING_EPOCHS, validation_data=val_batches)\n",
        "    # transformer.save(model_save_path)"
      ],
      "metadata": {
        "id": "RaPe4hb0neYP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "0PNAS3nDuRSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.deocder.last_attn_scores\n",
        "\n",
        "    return text, tokens, attention_weights\n",
        "\n",
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ],
      "metadata": {
        "id": "FoFQ0RoSuPT5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ],
      "metadata": {
        "id": "dlf_dlsL4C81"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'este é um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV2NZ3Vm4W00",
        "outputId": "05c16059-4c59-4892-db60-b431ca257774"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : este é um problema que temos que resolver.\n",
            "Prediction     : and i ' m going to think about the world .\n",
            "Ground truth   : this is a problem we have to solve .\n"
          ]
        }
      ]
    }
  ]
}